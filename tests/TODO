By nature, StartsWithFilter and EndsWithFilter are slow (indexing doesn't help much).
  Add a custom indexing process for these filters?

Implement composite ("sequential", "covering") index that supports multiple columns.
  Implement FunnelFilter.
    Only works on indexed columns.
    Should this be part of AndFilter rather than its own thing?
    Ideally would be generic so that StringRangeFilter and IntRangeFilter can be passed to it.
  In IndexHelper, allow user to specify a list of column names
    for an index rather than having to explicitly invoke save_funnel_index.

Develop compression scheme for string columns.

Modify column names to be private except a few.



CADD files:
  Rebuild these files.
  Parse
  Compare against tabix. We can also compare it in terms of size.
    https://www.htslib.org/doc/tabix.html
    tabix sorted.gff.gz chr1:10,000,000-20,000,000
  Try CSI index files https://github.com/enasequence/schema/issues/18
  Generate formal statistics on results.
  If we need to go faster to beat tabix:
    Try Nuitka. https://nuitka.net (compiles your Python code to C, is supposed to achieve speedups of 3x or greater).
    Parallelize the process of generating the output file (save to temp file)?
      This *might* help when doing a lot of decompression.

Test LINCS data (see WishBuilder).

Test refine.bio compendia.

Remove unnecessary comments and address TODO items in ../f4py/*.

Kick off full set of tests for large files and save results to GitHub.

Compare against other libraries/formats.
  Update to the latest version of R (in the Dockerfile).
  Update to the latest version of Python (in the Dockerfile)?

Implement basic http server functionality as a demo.

Modify name? Put Parallel(ize) in it?

#####################################################
# After benchmark paper (for software packages)
#####################################################

Raise a friendly exception if the user tries to do an indexed query when the index has not been created.

Allow user to specify missing values in Builder.convert() function. There is a function called is_missing_value in Utilities.py that should be removed.

Support the following scenarios (make sure the tests reflect them):
  1) uncompressed data + no index
  2) non-compressed data + non-compressed index
  3) compressed data + non-compressed index
    # See tests in TestSmallAndMedium.py
    # Update TestBuildLarge.py and TestParseLarge.py
  4) compressed data + compressed index (when they want to be able to query any column).

Bring back the filters that I commented out in Filters.py.
  Make sure we have tests for them in TestSmall.py.
  Also NumericWithoutFilter, NumericBetweenFilter, NumericEqualsFilter, NumericNotEqualsFilter

Expand NumericFilter to LessThanFilter, GreaterThanFilter, etc.
Add StringGreaterThanFilter, StringLessThanFilter (already have StringGreaterThanOrEqualsFilter, etc.)

The current design supports filtering on all non-indexed columns or all indexed
  columns but not a combination of both. What to do if the user violates this?
  Maybe if there is not an index for all filter columns, we revert to the slow
  version and just give them a warning.
  Maybe also optimize this code by having a flag file that indicates whether any
  column is indexed.

If you have a column that has mostly numeric data but has a few non-numeric values, the code to check the type might store tons of numbers as numeric values. Tweak the code to put a cap on how many can be stored in the set.

Store discrete data in separate table from numeric? The first column is a JSON dictionary that maps Unicode characters to unique values in the column. This could save a lot of disk space in most cases.
  When inferring column sizes and types, store missing values using a single character and replace them when the values are queried?
Make sure all arguments to public functions are fully validated in tests.
If possible, move functions out of Helper.py. If you keep any, document them.
Make all code consistent with PEP8 spec (using PyCharm).
Rework the format so that everything is stored in a single file.

Look at pandas for other functionality that might make sense to support? Add to TODO list.
Support joins?
Support date and string (has at least one non-number and more than 50% of values are unique?) columns.
Support in_file_delimiter="," and out_file_type="csv"
  Change in_file_delimiter to in_file_type?
  Make sure exception when invalid value specified.
GitHub builds for continuous integration and running the tests.
Add documentation for all public functions.
  Mention .gz file support.
  Mention that if they specify tmp_dir_path, we assume that it is empty. It will not work if it is not.
Set up readthedocs.

Build a FastAPI wrapper.

Translate to Rust. Or create Rust/Python wrapper. https://crates.io/crates/pyo3
  https://www.youtube.com/watch?v=Ygk0IMbu2nY
  https://dev-notes.eu/2020/03/Binary-Search-in-Rust/
  B-tree https://doc.rust-lang.org/std/collections/struct.BTreeMap.html
  https://docs.rs/sled/0.34.6/sled/struct.Tree.html

#####################################################
# May or may not do (at least for now)
#####################################################

Add function to Parser.py to return all unique values for a discrete column
Add function to Parser.py to get summary statistics for a numeric column
Support conversion from pandas DataFrame to F4 and vice versa
Provide explicit support for VCF format? Other bio formats?
Provide a way to stream a file as input and/or output?
