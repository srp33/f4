When we invoke _parse_row_value, pass line_length and file handle from outside the function to improve efficiency?
  Rerun TestParseLarge.py when other stuff not being executed on the server.

Print to standard out rather than saving directly to output file.

Rework parallelization when index column is used. Parallelize pulling row indices from index file (when there are a lot of them) in addition to (or rather than) for each sub-filter.

[Probably not] Rework Filters.filter_indexed_column_values_parallel for NumericWithinFilter so that the sub-filters do not store the indices for the entire ranges but rather just the limits of the range. Probably implement private classes called __NumericRangeLowerFilter and __NumericRangeUpperFilter.



CADD files:
  Rebuild these files.
  Parse
  Compare against tabix. We can also compare it in terms of size.
    https://www.htslib.org/doc/tabix.html
    tabix sorted.gff.gz chr1:10,000,000-20,000,000
  Try CSI index files https://github.com/enasequence/schema/issues/18
  Generate formal statistics on results.
  If we need to go faster to beat tabix:
    Implement composite ("covering", "cascading"?) index that supports multiple columns.
    Try Nuitka. https://nuitka.net (compiles your Python code to C, is supposed to achieve speedups of 3x or greater).


Do demo/tests on GNOMAD? LINCS data (see WishBuilder)?

Remove unnecessary comments and address TODO items in ../f4py/*.
  Do this in VS Studio Code or PyCharm?

Compare against other libraries/formats.
  Update to the latest version of R (in the Dockerfile).
  Update to the latest version of Python (in the Dockerfile)?

Implement basic http server functionality as a demo.

#####################################################
# After benchmark paper (for software packages)
#####################################################

Allow user to specify missing values in Builder.convert() function. There is a function called is_missing_value in Utilities.py that should be removed.

Support the following scenarios (make sure the tests reflect them):
  1) uncompressed data + no index
  2) non-compressed data + non-compressed index
  3) compressed data + non-compressed index
    # See tests in TestSmallAndMedium.py
    # Update TestBuildLarge.py and TestParseLarge.py
  4) compressed data + compressed index (when they want to be able to query any column).

Bring back the filters that I commented out in Filters.py.
  Make sure we have tests for them in TestSmall.py.
  Also NumericWithoutFilter, NumericBetweenFilter, NumericEqualsFilter, NumericNotEqualsFilter

Expand NumericFilter to LessThanFilter, GreaterThanFilter, etc.

The current design supports filtering on all non-indexed columns or all indexed
  columns but not a combination of both. What to do if the user violates this?
  Maybe if there is not an index for all filter columns, we revert to the slow
  version and just give them a warning.
  Maybe also optimize this code by having a flag file that indicates whether any
  column is indexed.

If you have a column that has mostly numeric data but has a few non-numeric values, the code to check the type might store tons of numbers as numeric values. Tweak the code to put a cap on how many can be stored in the set.

Store discrete data in separate table from numeric? The first column is a JSON dictionary that maps Unicode characters to unique values in the column. This could save a lot of disk space in most cases.
  When inferring column sizes and types, store missing values using a single character and replace them when the values are queried?
Make sure all arguments to public functions are fully validated in tests.
If possible, move functions out of Helper.py. If you keep any, document them.
Make all code consistent with PEP8 spec (using PyCharm).
Rework the format so that everything is stored in a single file.

Look at pandas for other functionality that might make sense to support? Add to TODO list.
Support joins?
Support date and string (has at least one non-number and more than 50% of values are unique?) columns.
Support in_file_delimiter="," and out_file_type="csv"
  Change in_file_delimiter to in_file_type?
  Make sure exception when invalid value specified.
GitHub builds for continuous integration and running the tests.
Add documentation for all public functions.
  Mention .gz file support.
  Mention that if they specify tmp_dir_path, we assume that it is empty. It will not work if it is not.
Set up readthedocs.

Build a FastAPI wrapper.

Translate to Rust. Or create Rust/Python wrapper. https://crates.io/crates/pyo3
  https://www.youtube.com/watch?v=Ygk0IMbu2nY
  https://dev-notes.eu/2020/03/Binary-Search-in-Rust/
  B-tree https://doc.rust-lang.org/std/collections/struct.BTreeMap.html
  https://docs.rs/sled/0.34.6/sled/struct.Tree.html

#####################################################
# May or may not do (at least for now)
#####################################################

Add function to Parser.py to return all unique values for a discrete column
Add function to Parser.py to get summary statistics for a numeric column
Support conversion from pandas DataFrame to F4 and vice versa
Provide explicit support for VCF format? Other bio formats?
Provide a way to stream a file as input and/or output?
