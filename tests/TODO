Implement parallelization in Parser.py.

When we invoke _parse_row_value, pass line_length and file handle from outside the function to improve efficiency?

Rework Filters.filter_indexed_column_values_parallel for NumericWithinFilter so that the sub-filters do not store the indices for the entire ranges but rather just the limits of the range. Probably implement private classes called __NumericRangeLowerFilter and __NumericRangeUpperFilter.

Remove unnecessary comments and address TODO items in ../f4py/*.

Test parsing CADD files.
  Rebuild these files.
  Compare against tabix. We can also compare it in terms of size.
    https://www.htslib.org/doc/tabix.html
    tabix sorted.gff.gz chr1:10,000,000-20,000,000
  Move to test.sh and generate formal statistics on it?

If we need to go faster to beat tabix:
    Try Nuitka. https://nuitka.net (compiles your Python code to C, is supposed to achieve speedups of 3x or greater).

Do demo/tests on LINCS data (see WishBuilder)?

Remove unnecessary comments and address TODO items in ../f4py/*.
  Do this in VS Studio Code or PyCharm.

#####################################################
# After benchmark paper (for software packages)
#####################################################

Support the following scenarios (make sure the tests reflect them):
  1) uncompressed data + no index
  2) non-compressed data + non-compressed index
  3) compressed data + non-compressed index
    # See tests in TestSmallAndMedium.py
    # Update TestBuildLarge.py and TestParseLarge.py
  4) compressed data + compressed index (when they want to be able to query any column).

Bring back the filters that I commented out in Filters.py.
  Make sure we have tests for them in TestSmall.py.
  Also NumericWithoutFilter, NumericBetweenFilter, NumericEqualsFilter, NumericNotEqualsFilter

The current design supports filtering on all non-indexed columns or all indexed
  columns but not a combination of both. What to do if the user violates this?
  Maybe if there is not an index for all filter columns, we revert to the slow
  version and just give them a warning.
  Maybe also optimize this code by having a flag file that indicates whether any
  column is indexed.

Implement on-disk binary search for finding column indices based on names (Parser.py).
  This would be beneficial if we want to support datasets with more than 1 million columns.
  _get_column_meta function in Parser.py
  Rework the logic in the get_column_type_from_name() function in Parser.py to use the same logic.
  Rework the logic in the save() function in Indexer.py to use same logic as used in Parser.py for binary search.

Store discrete data in separate table from numeric? The first column is a JSON dictionary that maps Unicode characters to unique values in the column. This could save a lot of disk space in most cases.
  When inferring column sizes and types, store missing values using a single character and replace them when the values are queried?
Make sure all arguments to public functions are fully validated in tests.
If possible, move functions out of Helper.py. If you keep any, document them.
Make all code consistent with PEP8 spec (using PyCharm).
Rework the format so that everything is stored in a single file.

Look at pandas for other functionality that might make sense to support? Add to TODO list.
Support joins?
Support date and string (has at least one non-number and more than 50% of values are unique?) columns.
Support in_file_delimiter="," and out_file_type="csv"
  Change in_file_delimiter to in_file_type?
  Make sure exception when invalid value specified.
GitHub builds for continuous integration and running the tests.
Add documentation for all public functions.
  Mention .gz file support.
  Mention that if they specify tmp_dir_path, we assume that it is empty. It will not work if it is not.
Set up readthedocs.

Build a FastAPI wrapper.

Translate to Rust. Or create Rust/Python wrapper. https://crates.io/crates/pyo3
  https://www.youtube.com/watch?v=Ygk0IMbu2nY
  https://dev-notes.eu/2020/03/Binary-Search-in-Rust/
  B-tree https://doc.rust-lang.org/std/collections/struct.BTreeMap.html
  https://docs.rs/sled/0.34.6/sled/struct.Tree.html

#####################################################
# May or may not do (at least for now)
#####################################################

Add function to Parser.py to return all unique values for a discrete column
Add function to Parser.py to get summary statistics for a numeric column
Support conversion from pandas DataFrame to F4 and vice versa
Provide explicit support for VCF format? Other bio formats?
Provide a way to stream a file as input and/or output?
