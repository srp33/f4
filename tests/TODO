In Builder.py, add a setting to ignore lines starting with a specified comment character (default = "#").
CSV/TSV compression project:
  Build an index file that indicates start position of each line and compression dictionary.
    Make it JSON or msgpack so it is language agnostic?
  Compare different compression algorithms.
    gzip
    zstandard (with training dictionary or without)
    lz4
    etc.
    bgzip?
  Read up on bgzip. Why do they use 64 kb blocks?
  Fine tune my method:
    We get poor performance when using n-grams only on cadd_head_medium. What is the optimal threshold for the proportion of unique values in a given column before you use n-grams?
      There seems to be a slight benefit when using lookup table with large integers. But how different is that from using n-grams on those?
    Evaluate best n size for n-grams. Automatically determine whether to use n-grams or dictionary lookup.
    Compare against encoding method.
    Describe scenarios (columns) where you get high vs. low compression ratios.
    Describe the benefits you get from bit packing.
    When converting bit arrays, can you ever get better compression ratios with zstandard than bitarray?
  What benefit do you get from gzipping the whole file in addition to a line-by-line scheme?
  Python code/module?
  R code/package?
  Lossy compression of numerical datasets using n-grams (see notes). (Or should that be a separate paper?)

Remove tmp_chunk_results_file_path logic from Builder.py (and function signature).
Remove the compression_level and build_compression_dict arguments?
Remove CompressionHelper.py?
Re-run TestBuildLarge.py overnight.
Try changing from pickle to msgpack (because it is language agnostic).
Add logic to only compress if the compressed version of a column will be smaller.

Remove unnecessary comments and address TODO items in ../f4py/*.

Huge benchmarks:
  CADD files:
    Figure out why compression is so poor.
      Does specifying too large a number matter for the training dict size?
      Does it make sense to build the training dictionary twice (once to determine size, then using that size).
      How much does compression_level matter?
    Compare against tabix. We can also compare it in terms of size.
      https://www.htslib.org/doc/tabix.html
      tabix sorted.gff.gz chr1:10,000,000-20,000,000
      Try CSI index files https://github.com/enasequence/schema/issues/18
    Pad every value with a space or a tab so the files are more readable?
      With compression this might not matter much?
  Gene-expression data:
    GSE62944
    ARCHS4 - https://maayanlab.cloud/archs4/download.html
      Human - TPM (transcript level) - 101 GB - HDF5 format
    ------------------------------------
    refine.bio - Normalized (not RNA-Seq) compendia - Homo sapiens (43 GB) - https://www.refine.bio/compendia?c=normalized
    LINCS (see WishBuilder)
    recount3 (must write code to download individual studies and then combine them)
  Generate formal statistics on results.
    How does the data size compare with and without training dict and for level 1 vs. 22?
  If we need to go faster to beat tabix:
    Try Nuitka. https://nuitka.net (compiles your Python code to C, is supposed to achieve speedups of 3x or greater).
    Parallelize the process of generating the output file (save to temp file)?
      This *might* help when doing a lot of decompression.
    Python 3.11. beta version already available. Production release in October 22. At least 20% speedup.

Remove unnecessary comments and address TODO items in ../f4py/*.

Kick off full set of tests for large files and save results to GitHub.

Compare against other libraries/formats.
  Update to the latest version of R (in the Dockerfile).
  Update to the latest version of Python (in the Dockerfile)?

Implement basic http server functionality as a demo.

Modify name? Put Parallel(ize) in it?

#####################################################
# After benchmark paper (for software packages)
#####################################################

Allow user to specify a number of decimal places in brackets or maybe just a string formatting text that will say how an indexed column should be formatted. This way you could have a float column that only needs one decimal point. Precision in this saves a lot of space.

Raise a friendly exception if the user tries to do an indexed query when the index has not been created.

Allow user to specify missing values in Builder.convert() function. There is a function called is_missing_value in Utilities.py that should be removed.

Implement interpolation search in place of binary search?
  https://softwareengineering.stackexchange.com/questions/119703/interpolation-search-vs-binary-search#119736

Support the following scenarios (make sure the tests reflect them):
  1) uncompressed data + no index
  2) non-compressed data + non-compressed index
  3) compressed data + non-compressed index
    # See tests in TestSmallAndMedium.py
    # Update TestBuildLarge.py and TestParseLarge.py
  4) compressed data + compressed index (when they want to be able to query any column).

Modify some function names to be private.

Consider using this: https://github.com/textualize/rich-cli

Bring back the filters that I commented out in Filters.py.
  Make sure we have tests for them in TestSmall.py.
  Also NumericWithoutFilter, NumericBetweenFilter, NumericEqualsFilter, NumericNotEqualsFilter

Expand NumericFilter to LessThanFilter, GreaterThanFilter, etc.
Add StringGreaterThanFilter, StringLessThanFilter (already have StringGreaterThanOrEqualsFilter, etc.)

The current design supports filtering on all non-indexed columns or all indexed
  columns but not a combination of both. What to do if the user violates this?
  Maybe if there is not an index for all filter columns, we revert to the slow
  version and just give them a warning.
  Maybe also optimize this code by having a flag file that indicates whether any
  column is indexed.

If you have a column that has mostly numeric data but has a few non-numeric values, the code to check the type might store tons of numbers as numeric values. Tweak the code to put a cap on how many can be stored in the set.

Store discrete data in separate table from numeric? The first column is a JSON dictionary that maps Unicode characters to unique values in the column. This could save a lot of disk space in most cases.
  When inferring column sizes and types, store missing values using a single character and replace them when the values are queried?
Make sure all arguments to public functions are fully validated in tests.
If possible, move functions out of Helper.py. If you keep any, document them.
Make all code consistent with PEP8 spec (using PyCharm).

Rework the format so that everything is stored in a single file.
  Use this spec? https://tools.ietf.org/id/draft-kunze-bagit-16.html

Look at pandas for other functionality that might make sense to support? Add to TODO list.
Support joins?
Support date and string (has at least one non-number and more than 50% of values are unique?) columns.
Support in_file_delimiter="," and out_file_type="csv"
  Change in_file_delimiter to in_file_type?
  Make sure exception when invalid value specified.
GitHub builds for continuous integration and running the tests.
Add documentation for all public functions.
  Mention .gz file support.
  Mention that if they specify tmp_dir_path, we assume that it is empty. It will not work if it is not.
Set up readthedocs.

Build a FastAPI wrapper.

Translate to Rust. Or create Rust/Python wrapper. https://crates.io/crates/pyo3
  https://www.youtube.com/watch?v=Ygk0IMbu2nY
  https://dev-notes.eu/2020/03/Binary-Search-in-Rust/
  B-tree https://doc.rust-lang.org/std/collections/struct.BTreeMap.html
  https://docs.rs/sled/0.34.6/sled/struct.Tree.html

#####################################################
# May or may not do (at least for now)
#####################################################

Add function to Parser.py to return all unique values for a discrete column
Add function to Parser.py to get summary statistics for a numeric column
Support conversion from pandas DataFrame to F4 and vice versa
Provide explicit support for VCF format? Other bio formats?
Provide a way to stream a file as input and/or output?
